{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Counts processing\n",
    "This is required for Huffman Encoding to count the apperances of each of the utf-8 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_counts(data):\n",
    "    #returns a dictionary of single character counts\n",
    "    #inputs: data: string\n",
    "    #outputs: dictionary of single character counts\n",
    "    single_counts = {}\n",
    "    for char in data:\n",
    "        #encode char in utf-8 \n",
    "        if char not in single_counts:\n",
    "            single_counts[char] = 0\n",
    "        single_counts[char] += 1\n",
    "    print(f\"single_counts: {single_counts}\")\n",
    "    return single_counts\n",
    "        \n",
    "\n",
    "def get_n_counts(data, n, debug = False):\n",
    "    n_counts = {}\n",
    "    for i in range(len(data)-n+1):\n",
    "        # if debug:\n",
    "        #     print(f\"i: {i}, data[i:i+n]: {data[i:i+n]}\")\n",
    "        n_gram = data[i:i+n]\n",
    "        if n_gram not in n_counts:\n",
    "            n_counts[n_gram] = 0\n",
    "        n_counts[n_gram] += 1\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"{n}-gram counts: {n_counts}\")\n",
    "    return n_counts\n",
    "\n",
    "def get_disjoint_n_counts(data, n, debug=False):\n",
    "    n_counts = {}\n",
    "    #get n-gram counts for disjoint blocks of the data size n and final step we just get the remainder gram\n",
    "    for i in range(0, len(data)-n+1, n):\n",
    "        n_gram = data[i:i+n]\n",
    "        if n_gram not in n_counts:\n",
    "            n_counts[n_gram] = 0\n",
    "        n_counts[n_gram] += 1\n",
    "    #final step\n",
    "    if len(data)%n != 0:\n",
    "        n_gram = data[-(len(data)%n):]\n",
    "        if n_gram not in n_counts:\n",
    "            n_counts[n_gram] = 0\n",
    "        n_counts[n_gram] += 1\n",
    "    return n_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Huffman Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tree class:\n",
    "from requests import get\n",
    "\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self, left, right, freq, value= None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.freq = freq\n",
    "        self.value = value\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Tree({self.freq}, {self.value}, {self.left}, {self.right})\"\n",
    "    \n",
    "\n",
    "class Huffman:\n",
    "    def __init__(self, counts =1, disjoint = True):\n",
    "        #Huffman Model, including encoder and decoder, codebook difference, and compression ratio\n",
    "        #data: string to be encoded\n",
    "        #counts: number of characters to encode at a time, default 1\n",
    "        #disjoint: boolean to encode disjoint blocks of the data, default False\n",
    "        self.huffman_tree = None\n",
    "        self.codebook = {}\n",
    "        self.counts = counts\n",
    "        self.counts_dict = {}\n",
    "        self.disjoint = disjoint\n",
    "        \n",
    "    def run(self, data, file_name=None, debug = False):\n",
    "        if file_name is not None:\n",
    "            print(f\"-------------------\\nRunning Huffman Model on input file: {file_name} of length: {len(data)}\\n-------------------\")\n",
    "        \n",
    "        else: print(f\"---------------------\\nRunning Huffman model on data of length {len(data)}\\n---------------------\")\n",
    "        print(f\"counts: {self.counts}, disjoint: {self.disjoint}\")\n",
    "        if debug:\n",
    "            print(f\"counts: {self.counts}, disjoint: {self.disjoint}\")\n",
    "        if len(data) < self.counts:\n",
    "            print(f\"Data length must be greater than {self.counts}, as the model uses {self.counts}-grams\")\n",
    "            return None\n",
    "        code = self.encode(data, debug)\n",
    "        output = self.decode(code, debug)  \n",
    "        print(f\"Input-Ouput Verification: {output == data}\") \n",
    "        print(f\"Compression ratio: {self.get_compression_ratio(data, code)}\")\n",
    "        try: \n",
    "            if debug.lower() == \"verbose\":\n",
    "                print(f\"code: {code}\")\n",
    "                print(f\"Input: {data}\\nOutput: {output}\")\n",
    "\n",
    "        except:\n",
    "            if debug:\n",
    "                print(f\"Input: {data}\\nOutput: {output}\")\n",
    "      \n",
    "        \n",
    "    def encode(self, data, debug = False):\n",
    "        #encode the data using the huffman tree\n",
    "        #inputs: data: string of the data to be encoded\n",
    "        #        single_counts: dictionary of single character counts\n",
    "        #outputs: encoded data\n",
    "\n",
    "        self.get_counts(data,debug) #obtains self.counts_dict\n",
    "        if debug:\n",
    "            print(f\"{self.counts}-gram counts: {self.counts_dict}\")\n",
    "        self.generate_huffman_tree(debug=debug) #obtains self.huffman_tree\n",
    "        if debug:\n",
    "            print(f\"huffman_tree: {self.huffman_tree}\")\n",
    "        self.generate_codebook() #obtains self.codebook\n",
    "        if debug:\n",
    "            print(f\"encoder_codebook: {self.codebook}\")\n",
    "        code = self.get_code(data) #obtains code\n",
    "        return code\n",
    "\n",
    "    def get_code(self, data):\n",
    "        #returns the code of the data\n",
    "        #inputs: data: string of the data to be encoded\n",
    "        #        huffman_tree: the huffman tree to encode the data\n",
    "        #outputs: code: encoded data\n",
    "        if self.counts == 1 or not self.disjoint:\n",
    "            code = \"\"\n",
    "            #encode n-gramwise\n",
    "            for i in range(len(data)-self.counts +1):\n",
    "                n_gram = data[i:i+self.counts]\n",
    "                code += self.codebook[n_gram]\n",
    "            \n",
    "            return code\n",
    "        else: #n-gramwise disjoint\n",
    "            code = \"\"\n",
    "            #encode with n-gramwise jumps\n",
    "            for i in range(0, len(data), self.counts):\n",
    "                n_gram = data[i:i+self.counts]\n",
    "                code += self.codebook[n_gram]\n",
    "            #final step\n",
    "            return code\n",
    "    \n",
    "        \n",
    "    def generate_codebook(self):\n",
    "        #generate codebook from self.huffman_tree   \n",
    "\n",
    "        def create_encoding(tree, code):\n",
    "            if tree.value:\n",
    "                self.codebook[tree.value] = code\n",
    "            else:\n",
    "                create_encoding(tree.left, code + '0')\n",
    "                create_encoding(tree.right, code + '1')\n",
    "        create_encoding(self.huffman_tree, '')\n",
    "        \n",
    "    \n",
    "    def generate_huffman_tree(self, debug = False):\n",
    "        if debug:\n",
    "            print(f\"counts_dict: {self.counts_dict}\")\n",
    "        trees = [Tree(None, None, self.counts_dict[key], key) for key in self.counts_dict]\n",
    "        #2. sort the list based on frequency\n",
    "        trees.sort()\n",
    "\n",
    "        #3. while there is more than one tree in the list:\n",
    "        while len(trees) > 1:\n",
    "            #3.1 take the first two trees from the list and create a new tree with them as children. \n",
    "            #The new tree's frequency is the sum of the two trees' frequencies.\n",
    "            #The two frequencies guaranteed to be the lowest two\n",
    "            left = trees.pop(0)\n",
    "            right = trees.pop(0)\n",
    "            new_tree = Tree(left, right, left.freq + right.freq)\n",
    "            #3.2 add the new tree to the list\n",
    "            trees.append(new_tree)\n",
    "            #3.3 re-sort the list for the priority queue to have the lowest frequency at the top\n",
    "            trees.sort()\n",
    "            \n",
    "        #4. the tree left in the list is the huffman tree\n",
    "        self.huffman_tree = trees[0]\n",
    "        \n",
    "    def decode(self, code, debug = False):\n",
    "        #decode the code using the huffman tree\n",
    "        #inputs: code: string of the code to be decoded\n",
    "        #        huffman_tree: the huffman tree to decode the code\n",
    "        #outputs: decoded data\n",
    "        if self.counts == 1 or not self.disjoint:\n",
    "            decoded_data = \"\"\n",
    "            current_code = \"\"\n",
    "            inverse_codebook = {v: k for k, v in self.codebook.items()}\n",
    "            n_gram_size = self.counts  # Adjust for n-grams\n",
    "            \n",
    "            for bit in code:\n",
    "                current_code += bit\n",
    "                if current_code in inverse_codebook.keys():\n",
    "                    if n_gram_size == 1:\n",
    "                        decoded_data += inverse_codebook[current_code]\n",
    "                        current_code = \"\"\n",
    "                    else:\n",
    "                        decoded_data  = decoded_data[:-n_gram_size+1] + inverse_codebook[current_code]\n",
    "                        current_code = \"\"\n",
    "            return decoded_data\n",
    "        else: #disjoint\n",
    "            decoded_data = \"\"\n",
    "            current_code = \"\"\n",
    "            inverse_codebook = {v: k for k, v in self.codebook.items()}\n",
    "            n_gram_size = self.counts  # Adjust for n-grams\n",
    "            for bit in code:\n",
    "                current_code += bit\n",
    "                \n",
    "                if current_code in inverse_codebook.keys():\n",
    "                    if debug:\n",
    "                        print(f\"current_code: {current_code}\")\n",
    "                    decoded_data += inverse_codebook[current_code]\n",
    "                    current_code = \"\"\n",
    "                    if debug:\n",
    "                        print(f\"decoded_data: {decoded_data}\")\n",
    "            return decoded_data\n",
    "                    \n",
    "        \n",
    "    \n",
    "    def get_counts(self, data, debug = False):\n",
    "        #returns a dictionary of character counts based on self.counts\n",
    "        #inputs: data: string\n",
    "        #outputs: dictionary of self.counts # character counts\n",
    "        if self.counts ==1 or not self.disjoint:\n",
    "            if debug:\n",
    "                print(f\"get n counts\")\n",
    "            self.counts_dict = get_n_counts(data, self.counts, debug)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"get disjoint n counts\")\n",
    "            self.counts_dict = get_disjoint_n_counts(data, self.counts, debug)\n",
    "        \n",
    "        \n",
    "    def get_compression_ratio(self, data, code):\n",
    "        num_bits_data_utf8 = len(data.encode('utf-8'))*8\n",
    "        num_bits_dictionary = sum([len(self.codebook[key]) + len(key.encode('utf-8'))*8 for key in self.codebook]) \n",
    "        #value is in binary, key is in utf-8, we add key + value bits for each key.\n",
    "        num_bits_code = len(code) #code is already in bits.\n",
    "        print(f\"num_bits_data_utf8: {num_bits_data_utf8}, num_bits_code: {num_bits_code} num_bits_dictionary: {num_bits_dictionary}\")\n",
    "        compression_ratio = num_bits_data_utf8/ (num_bits_code+ num_bits_dictionary) \n",
    "        return compression_ratio\n",
    "    \n",
    "    def get_codebook_difference(self, encoder_codebook, decoder_codebook):\n",
    "        #returns the difference between the codebooks of the encoder and decoder\n",
    "        #returns a dictionary of the differences   \n",
    "        difference = {}\n",
    "        for key in encoder_codebook:\n",
    "            if key not in decoder_codebook.values():\n",
    "                difference[key] = encoder_codebook[key]\n",
    "        return difference\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "    def run_onfile(self, filename, debug = False):\n",
    "        #runs the Huffman model on the input file\n",
    "        #returns the compression ratio\n",
    "        #Inputs: filename: string of the file to be encoded \n",
    "        #        debug [True, False, \"Verbose\"]: boolean to print debug statements\n",
    "        #Prints: Input-Ouput Verification: True if the input and output are the same, False otherwise\n",
    "        #        Compression ratio: the compression ratio of the input and output\n",
    "        #        code: the encoded data\n",
    "        #        encoder_codebook: the codebook of the encoder\n",
    "        #        decoder_codebook: the codebook of the decoder\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = f.read()\n",
    "        self.run(data, filename, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade off for larger n-grams\n",
    "Although it might sound good since we can output all in one codebook, our worst case if data is a sequence of unique n-nary elements, our codebook needs to store the each of the elements size of n, which means it's uncompressed.\n",
    "### Observation Disjoint $\\geq$ sliding window:\n",
    "Implementation issue of sliding window is just inefficient since we have ```len(data)-n+1``` codewords no matter what instead of ```roof((len(data)/n))```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Running Huffman model on data of length 256\n",
      "---------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 3072, num_bits_code: 2048 num_bits_dictionary: 5120\n",
      "Compression ratio: 0.42857142857142855\n",
      "---------------------\n",
      "Running Huffman model on data of length 24\n",
      "---------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 192, num_bits_code: 40 num_bits_dictionary: 194\n",
      "Compression ratio: 0.8205128205128205\n",
      "---------------------\n",
      "Running Huffman model on data of length 24\n",
      "---------------------\n",
      "counts: 3, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 192, num_bits_code: 13 num_bits_dictionary: 77\n",
      "Compression ratio: 2.1333333333333333\n",
      "---------------------\n",
      "Running Huffman model on data of length 24\n",
      "---------------------\n",
      "counts: 8, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 192, num_bits_code: 5 num_bits_dictionary: 197\n",
      "Compression ratio: 0.9504950495049505\n"
     ]
    }
   ],
   "source": [
    "#test on 1,2,3 counts same as LZW\n",
    "# Single counts:\n",
    "import dis\n",
    "\n",
    "\n",
    "huffman = Huffman(counts = 1)\n",
    "huffman.run(\"\".join([chr(i) for i in range(256)]))\n",
    "# huffman.run(\"BABAABAAA\")\n",
    "# 2-gram counts:\n",
    "huffman = Huffman(counts = 2, disjoint = True)\n",
    "# huffman.run(\"AAB\", debug = \"verbose\")   \n",
    "\n",
    "huffman.run(\"TOBEORNOTTOBEORTOBEORNOT\")\n",
    "# huffman.run(\"BABAABAAA\")\n",
    "# # # 3-gram counts:\n",
    "huffman = Huffman(counts = 3, disjoint =True)\n",
    "huffman.run(\"TOBEORNOTTOBEORTOBEORNOT\")\n",
    "# huffman.run(\"BABAABAAA\")\n",
    "huffman = Huffman(counts = 8, disjoint =True)\n",
    "huffman.run(\"TOBEORNOTTOBEORTOBEORNOT\")\n",
    "# huffman.run(\"BABAABAAA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000\n",
    "data = ''.join(np.random.choice(['A','B'], size=n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Running Huffman model on data of length 10000\n",
      "---------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 80000, num_bits_code: 10000 num_bits_dictionary: 18\n",
      "Compression ratio: 7.98562587342783\n",
      "---------------------\n",
      "Running Huffman model on data of length 10000\n",
      "---------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 80000, num_bits_code: 10000 num_bits_dictionary: 72\n",
      "Compression ratio: 7.942811755361398\n",
      "---------------------\n",
      "Running Huffman model on data of length 10000\n",
      "---------------------\n",
      "counts: 5, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 80000, num_bits_code: 10000 num_bits_dictionary: 1440\n",
      "Compression ratio: 6.993006993006993\n",
      "---------------------\n",
      "Running Huffman model on data of length 10000\n",
      "---------------------\n",
      "counts: 10, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 80000, num_bits_code: 9202 num_bits_dictionary: 58184\n",
      "Compression ratio: 1.1871902175526075\n",
      "---------------------\n",
      "Running Huffman model on data of length 10000\n",
      "---------------------\n",
      "counts: 100, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 80000, num_bits_code: 672 num_bits_dictionary: 80672\n",
      "Compression ratio: 0.983477576711251\n"
     ]
    }
   ],
   "source": [
    "Huffman(counts= 1, disjoint = True).run(data)\n",
    "Huffman(counts = 2, disjoint = True).run(data)\n",
    "Huffman(counts = 5, disjoint = True).run(data)\n",
    "Huffman(counts = 10, disjoint = True).run(data)\n",
    "Huffman(counts = 100, disjoint = True).run(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Huffman on filtered datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/principia_mathematica.txt of length: 804324\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 6434592, num_bits_code: 3745936 num_bits_dictionary: 1469\n",
      "Compression ratio: 1.7170794189579188\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Bible.txt of length: 4332587\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 34660696, num_bits_code: 19827130 num_bits_dictionary: 1521\n",
      "Compression ratio: 1.7480107950863626\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/beowulf.txt of length: 273007\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 2184056, num_bits_code: 1226832 num_bits_dictionary: 1520\n",
      "Compression ratio: 1.7780375657791903\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/theorie_des_distributions.txt of length: 275644\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 2205152, num_bits_code: 1342802 num_bits_dictionary: 1607\n",
      "Compression ratio: 1.640238945142438\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Myth_of_Sisyphus.txt of length: 113168\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 905344, num_bits_code: 497791 num_bits_dictionary: 1703\n",
      "Compression ratio: 1.8125222725398102\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/hamlet.txt of length: 177888\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 1423104, num_bits_code: 849057 num_bits_dictionary: 1618\n",
      "Compression ratio: 1.6729115114467923\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/around_the_world.txt of length: 2519\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 20152, num_bits_code: 9426 num_bits_dictionary: 1610\n",
      "Compression ratio: 1.8260239217107648\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/God_save_the_king.txt of length: 327\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2616, num_bits_code: 1476 num_bits_dictionary: 1610\n",
      "Compression ratio: 0.847699287103046\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/magic_flute.txt of length: 49723\n",
      "-------------------\n",
      "counts: 1, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 397784, num_bits_code: 242450 num_bits_dictionary: 1634\n",
      "Compression ratio: 1.6297012503892103\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/principia_mathematica.txt of length: 804324\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 6434592, num_bits_code: 3331620 num_bits_dictionary: 74580\n",
      "Compression ratio: 1.8890822617579708\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Bible.txt of length: 4332587\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 34660696, num_bits_code: 17156049 num_bits_dictionary: 84239\n",
      "Compression ratio: 2.0104476212926374\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/beowulf.txt of length: 273007\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2184056, num_bits_code: 1083124 num_bits_dictionary: 95286\n",
      "Compression ratio: 1.8533922828217684\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/theorie_des_distributions.txt of length: 275644\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2205152, num_bits_code: 1170836 num_bits_dictionary: 131319\n",
      "Compression ratio: 1.6934635277674317\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Myth_of_Sisyphus.txt of length: 113168\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 905344, num_bits_code: 442626 num_bits_dictionary: 132173\n",
      "Compression ratio: 1.5750618912002283\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/hamlet.txt of length: 177888\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 1423104, num_bits_code: 734331 num_bits_dictionary: 132096\n",
      "Compression ratio: 1.6424972905968998\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/around_the_world.txt of length: 2519\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 20152, num_bits_code: 5544 num_bits_dictionary: 132037\n",
      "Compression ratio: 0.14647371366685807\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/God_save_the_king.txt of length: 327\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2616, num_bits_code: 1039 num_bits_dictionary: 131896\n",
      "Compression ratio: 0.019678790386279007\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/magic_flute.txt of length: 49723\n",
      "-------------------\n",
      "counts: 2, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 397784, num_bits_code: 205764 num_bits_dictionary: 132224\n",
      "Compression ratio: 1.1769175236990663\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/principia_mathematica.txt of length: 804324\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 6434592, num_bits_code: 2613359 num_bits_dictionary: 1481492\n",
      "Compression ratio: 1.571386113926978\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Bible.txt of length: 4332587\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 34660696, num_bits_code: 13440213 num_bits_dictionary: 3152230\n",
      "Compression ratio: 2.0889447081421344\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/beowulf.txt of length: 273007\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2184056, num_bits_code: 824241 num_bits_dictionary: 3467645\n",
      "Compression ratio: 0.5088802451882459\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/theorie_des_distributions.txt of length: 275644\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2205152, num_bits_code: 858576 num_bits_dictionary: 4051796\n",
      "Compression ratio: 0.44908043626837235\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Myth_of_Sisyphus.txt of length: 113168\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 905344, num_bits_code: 339424 num_bits_dictionary: 4097644\n",
      "Compression ratio: 0.2040410469255824\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/hamlet.txt of length: 177888\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 1423104, num_bits_code: 558544 num_bits_dictionary: 4208839\n",
      "Compression ratio: 0.2985084269503835\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/around_the_world.txt of length: 2519\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: True\n",
      "num_bits_data_utf8: 20152, num_bits_code: 2844 num_bits_dictionary: 4208775\n",
      "Compression ratio: 0.004784858269468345\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/God_save_the_king.txt of length: 327\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 2616, num_bits_code: 514 num_bits_dictionary: 4208368\n",
      "Compression ratio: 0.000621542727973842\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/music/magic_flute.txt of length: 49723\n",
      "-------------------\n",
      "counts: 4, disjoint: True\n",
      "Input-Ouput Verification: False\n",
      "num_bits_data_utf8: 397784, num_bits_code: 146417 num_bits_dictionary: 4243336\n",
      "Compression ratio: 0.09061648798918755\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/principia_mathematica.txt of length: 804324\n",
      "-------------------\n",
      "counts: 8, disjoint: True\n",
      "File: filtered_datasets/books/principia_mathematica.txt failed on n_gram size: 8\n",
      "-------------------\n",
      "Running Huffman Model on input file: filtered_datasets/books/Bible.txt of length: 4332587\n",
      "-------------------\n",
      "counts: 8, disjoint: True\n"
     ]
    }
   ],
   "source": [
    "#run LZW on all files in filtered_datasets/books and filtered_datasets/music\n",
    "import os\n",
    "for n_gram in [1,2,4,8]:\n",
    "    \n",
    "    huffman = Huffman(counts = n_gram, disjoint = True)\n",
    "    for root, dirs, files in os.walk(\"filtered_datasets/books\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    huffman.run_onfile(file_path)\n",
    "                except:\n",
    "                    print(f\"File: {file_path} failed on n_gram size: {n_gram}\")\n",
    "            \n",
    "    for root, dirs, files in os.walk(\"filtered_datasets/music\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                try:\n",
    "                    huffman.run_onfile(file_path)\n",
    "                except:\n",
    "                    print(f\"File: {file_path} failed on n_gram size: {n_gram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
